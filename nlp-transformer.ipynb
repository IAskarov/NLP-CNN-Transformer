{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-27T13:47:37.017478Z","iopub.execute_input":"2023-05-27T13:47:37.017883Z","iopub.status.idle":"2023-05-27T13:47:37.027945Z","shell.execute_reply.started":"2023-05-27T13:47:37.017835Z","shell.execute_reply":"2023-05-27T13:47:37.026928Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-txt-classification/sample_submission.csv\n/kaggle/input/nlp-txt-classification/train.csv\n/kaggle/input/nlp-txt-classification/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:47:37.030028Z","iopub.execute_input":"2023-05-27T13:47:37.031015Z","iopub.status.idle":"2023-05-27T13:47:52.112277Z","shell.execute_reply.started":"2023-05-27T13:47:37.030959Z","shell.execute_reply":"2023-05-27T13:47:52.111030Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Помогает решить ошибки во время импорта \"transformers\"\n!pip install --upgrade accelerate","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:47:52.115036Z","iopub.execute_input":"2023-05-27T13:47:52.115702Z","iopub.status.idle":"2023-05-27T13:48:04.099767Z","shell.execute_reply.started":"2023-05-27T13:47:52.115663Z","shell.execute_reply":"2023-05-27T13:48:04.098573Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.19.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nimport os\nimport re\nimport torch\nimport evaluate\nfrom sklearn import preprocessing\nimport transformers\nfrom transformers import AutoTokenizer\nfrom transformers import Trainer\nfrom transformers import TrainingArguments\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    PretrainedConfig,\n    default_data_collator   \n)\n\nimport datasets\nfrom datasets import load_dataset\nfrom datasets import load_dataset_builder\nfrom datasets import Dataset\n\ntorch.cuda.is_available()\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nstemmer = PorterStemmer()\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:04.102706Z","iopub.execute_input":"2023-05-27T13:48:04.103091Z","iopub.status.idle":"2023-05-27T13:48:33.239634Z","shell.execute_reply.started":"2023-05-27T13:48:04.103057Z","shell.execute_reply":"2023-05-27T13:48:33.238724Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-txt-classification/train.csv').dropna()\ndf_test = pd.read_csv('/kaggle/input/nlp-txt-classification/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/nlp-txt-classification/sample_submission.csv')\nprint('Shape train: {}'.format(df_train.shape))\nprint('Shape test: {}'.format(df_test.shape))\nprint('Shape sample_submission: {}'.format(sample_submission.shape))\nprint('-'*40)\nprint('Gaps train: {}'.format(df_train.isnull().sum().sum()))\nprint('Gaps test: {}'.format(df_test.isnull().sum().sum()))\nprint('Gaps sample_submission: {}'.format(sample_submission.isnull().sum().sum()))","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:33.242407Z","iopub.execute_input":"2023-05-27T13:48:33.242847Z","iopub.status.idle":"2023-05-27T13:48:33.675015Z","shell.execute_reply.started":"2023-05-27T13:48:33.242814Z","shell.execute_reply":"2023-05-27T13:48:33.674061Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Shape train: (41155, 3)\nShape test: (3798, 2)\nShape sample_submission: (3798, 2)\n----------------------------------------\nGaps train: 0\nGaps test: 0\nGaps sample_submission: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:33.676272Z","iopub.execute_input":"2023-05-27T13:48:33.676898Z","iopub.status.idle":"2023-05-27T13:48:33.697195Z","shell.execute_reply.started":"2023-05-27T13:48:33.676854Z","shell.execute_reply":"2023-05-27T13:48:33.696133Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0                                               Text  \\\n11437      11437  If there was a God this virus would target all...   \n29867      29867  Amid to covid-19 lockdown, budget aquaman ress...   \n32707      32707  @ate_ted @SenTedCruz Supply &amp; Demand. COVI...   \n4854        4854  Empty store shelves in grocery store do to pan...   \n1217        1217  President announcement practise social distanc...   \n\n                Sentiment  \n11437  Extremely Negative  \n29867             Neutral  \n32707            Positive  \n4854             Negative  \n1217             Positive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Text</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11437</th>\n      <td>11437</td>\n      <td>If there was a God this virus would target all...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>29867</th>\n      <td>29867</td>\n      <td>Amid to covid-19 lockdown, budget aquaman ress...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>32707</th>\n      <td>32707</td>\n      <td>@ate_ted @SenTedCruz Supply &amp;amp; Demand. COVI...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4854</th>\n      <td>4854</td>\n      <td>Empty store shelves in grocery store do to pan...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>1217</td>\n      <td>President announcement practise social distanc...</td>\n      <td>Positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"LABELS = df_train['Sentiment'].unique()\nLABELS_COUNT = len(df_train['Sentiment'].unique())","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:33.698768Z","iopub.execute_input":"2023-05-27T13:48:33.699143Z","iopub.status.idle":"2023-05-27T13:48:33.714954Z","shell.execute_reply.started":"2023-05-27T13:48:33.699108Z","shell.execute_reply":"2023-05-27T13:48:33.714006Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Помогает решить ошибки для \"wordnet\"\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:33.716541Z","iopub.execute_input":"2023-05-27T13:48:33.716898Z","iopub.status.idle":"2023-05-27T13:48:35.018093Z","shell.execute_reply.started":"2023-05-27T13:48:33.716840Z","shell.execute_reply":"2023-05-27T13:48:35.016988Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text(text, url=False, lower=True, punct=True, stop=True, stem=False, lem=True):\n\n    # Очистка текста от URL-адресов\n    if url:\n        text = re.sub(r'http\\S+', '', text)\n    \n    # Приведение символов к нижнему регистру\n    if lower:\n        text = text.lower()\n\n    # Удаление пунктуации\n    if punct:\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n\n    # Удаление стоп-слов\n    if stop:\n        stop_words = set(stopwords.words('english'))\n        words = nltk.word_tokenize(text)\n        filtered_words = [word for word in words if word not in stop_words]\n        text = \" \".join(filtered_words)\n\n    # Стемминг\n    if stem:\n        porter_stemmer = PorterStemmer()\n        words = nltk.word_tokenize(text)\n        stemmed_words = [porter_stemmer.stem(word) for word in words]\n        text = \" \".join(stemmed_words)\n\n    # Лемматизация\n    if lem:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        words = nltk.word_tokenize(text)\n        lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n        text = \" \".join(lemmatized_words)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:35.019795Z","iopub.execute_input":"2023-05-27T13:48:35.020094Z","iopub.status.idle":"2023-05-27T13:48:35.032505Z","shell.execute_reply.started":"2023-05-27T13:48:35.020070Z","shell.execute_reply":"2023-05-27T13:48:35.031533Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train['Text'] = df_train['Text'].apply(preprocess_text)\ndf_test['Text'] = df_test['Text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:48:35.034069Z","iopub.execute_input":"2023-05-27T13:48:35.034688Z","iopub.status.idle":"2023-05-27T13:49:21.116963Z","shell.execute_reply.started":"2023-05-27T13:48:35.034656Z","shell.execute_reply":"2023-05-27T13:49:21.116002Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train.sample(3)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:21.121434Z","iopub.execute_input":"2023-05-27T13:49:21.121722Z","iopub.status.idle":"2023-05-27T13:49:21.141103Z","shell.execute_reply.started":"2023-05-27T13:49:21.121698Z","shell.execute_reply":"2023-05-27T13:49:21.140176Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0                                               Text  \\\n17344      17344  mondelez hiring 1 000 u employee help maintain...   \n25332      25332  enforcement ca consumer privacy act set start ...   \n15130      15130  philip ramping production ventilator really ne...   \n\n                Sentiment  \n17344            Positive  \n25332            Negative  \n15130  Extremely Negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Text</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17344</th>\n      <td>17344</td>\n      <td>mondelez hiring 1 000 u employee help maintain...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>25332</th>\n      <td>25332</td>\n      <td>enforcement ca consumer privacy act set start ...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>15130</th>\n      <td>15130</td>\n      <td>philip ramping production ventilator really ne...</td>\n      <td>Extremely Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Закодируем признак `Sentiment`\nle = preprocessing.LabelEncoder()\nle.fit(LABELS)\n\ndf_train['Sentiment'] = le.transform(df_train['Sentiment'])\ndf_train.rename(columns={'Sentiment': 'label'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:21.142689Z","iopub.execute_input":"2023-05-27T13:49:21.143065Z","iopub.status.idle":"2023-05-27T13:49:21.162666Z","shell.execute_reply.started":"2023-05-27T13:49:21.143032Z","shell.execute_reply":"2023-05-27T13:49:21.161740Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# tokenizer\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n                                                           num_labels=LABELS_COUNT,\n                                                           ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:21.163941Z","iopub.execute_input":"2023-05-27T13:49:21.164519Z","iopub.status.idle":"2023-05-27T13:49:25.642715Z","shell.execute_reply.started":"2023-05-27T13:49:21.164484Z","shell.execute_reply":"2023-05-27T13:49:25.641131Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e89974995ef4dc8ab04e2480dcc0852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da0309ba19214cbdbbf0637023a58eae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7478eb2b116347d39643217d30473088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e522164f8294a56abc82646ae08a046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85f8c5389a44cc6816a13b900768d8d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Запишем функцию\ndef tokenize_function(data):\n    return tokenizer(data[\"Text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:25.646100Z","iopub.execute_input":"2023-05-27T13:49:25.646450Z","iopub.status.idle":"2023-05-27T13:49:25.658434Z","shell.execute_reply.started":"2023-05-27T13:49:25.646420Z","shell.execute_reply":"2023-05-27T13:49:25.657316Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:25.659810Z","iopub.execute_input":"2023-05-27T13:49:25.660406Z","iopub.status.idle":"2023-05-27T13:49:25.684411Z","shell.execute_reply.started":"2023-05-27T13:49:25.660371Z","shell.execute_reply":"2023-05-27T13:49:25.683281Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  787bc85b-20d4-46d8-84a0-562a2527f684   \n1  17e934cd-ba94-4d4f-9ac0-ead202abe241   \n2  5914534b-2b0f-4de8-bb8a-e25587697e0d   \n3  cdf06cfe-29ae-48ee-ac6d-be448103ba45   \n4  aff63979-0256-4fb9-a2d9-86a3d3ca5470   \n\n                                                Text  \n0  trending new yorkers encounter empty supermark...  \n1  couldnt find hand sanitizer fred meyer turned ...  \n2                 find protect loved one coronavirus  \n3  panic buying hit newyork city anxious shopper ...  \n4  toiletpaper dunnypaper coronavirus coronavirus...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n      <td>trending new yorkers encounter empty supermark...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n      <td>couldnt find hand sanitizer fred meyer turned ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n      <td>find protect loved one coronavirus</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n      <td>panic buying hit newyork city anxious shopper ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Train\ntrain_dt = Dataset.from_pandas(df_train[['Text','label']])\ntokenized_train = train_dt.map(tokenize_function, batched=True)\ntokenized_train = tokenized_train.remove_columns([\"__index_level_0__\"])\n\n#test \ntest_dt = Dataset.from_pandas(df_test)\ntokenized_test = test_dt.map(tokenize_function, batched=True)\ntokenized_test = tokenized_test.remove_columns(['id', 'Text'])\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer) #padding funct.","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:25.690753Z","iopub.execute_input":"2023-05-27T13:49:25.691574Z","iopub.status.idle":"2023-05-27T13:49:31.661307Z","shell.execute_reply.started":"2023-05-27T13:49:25.691535Z","shell.execute_reply":"2023-05-27T13:49:31.660289Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/42 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe4460a1b744d7cae242b64bd40f15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4aaad82d0a0446c918c834be0f5a176"}},"metadata":{}}]},{"cell_type":"code","source":"splitted_data= tokenized_train.train_test_split(test_size=0.2)\nsplitted_data","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:31.662711Z","iopub.execute_input":"2023-05-27T13:49:31.663313Z","iopub.status.idle":"2023-05-27T13:49:31.690572Z","shell.execute_reply.started":"2023-05-27T13:49:31.663276Z","shell.execute_reply":"2023-05-27T13:49:31.689589Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 32924\n    })\n    test: Dataset({\n        features: ['Text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 8231\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"# metrics count function\ndef compute_metrics(eval_preds):\n    metric = evaluate.load('accuracy')\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:31.691917Z","iopub.execute_input":"2023-05-27T13:49:31.692264Z","iopub.status.idle":"2023-05-27T13:49:31.697926Z","shell.execute_reply.started":"2023-05-27T13:49:31.692232Z","shell.execute_reply":"2023-05-27T13:49:31.696787Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    \"test-trainer\",     \n    evaluation_strategy=\"epoch\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    optim=\"adamw_torch\",\n    warmup_steps=600,\n    weight_decay=0.01,\n    logging_steps=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:31.699617Z","iopub.execute_input":"2023-05-27T13:49:31.699982Z","iopub.status.idle":"2023-05-27T13:49:31.735471Z","shell.execute_reply.started":"2023-05-27T13:49:31.699951Z","shell.execute_reply":"2023-05-27T13:49:31.734572Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    training_args,\n    train_dataset=splitted_data['train'],\n    eval_dataset=splitted_data['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:31.736708Z","iopub.execute_input":"2023-05-27T13:49:31.737041Z","iopub.status.idle":"2023-05-27T13:49:40.328835Z","shell.execute_reply.started":"2023-05-27T13:49:31.737012Z","shell.execute_reply":"2023-05-27T13:49:40.327914Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"transformers.logging.set_verbosity('CRITICAL')\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-27T13:49:40.330139Z","iopub.execute_input":"2023-05-27T13:49:40.330600Z","iopub.status.idle":"2023-05-27T14:07:25.909820Z","shell.execute_reply.started":"2023-05-27T13:49:40.330567Z","shell.execute_reply":"2023-05-27T14:07:25.908766Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3087' max='3087' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3087/3087 17:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.820600</td>\n      <td>0.727061</td>\n      <td>0.741465</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.420000</td>\n      <td>0.708806</td>\n      <td>0.751185</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.254600</td>\n      <td>0.679502</td>\n      <td>0.778885</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42811b28d10c4627912b800b5fe7d168"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3087, training_loss=0.6911669404115884, metrics={'train_runtime': 1065.5544, 'train_samples_per_second': 92.695, 'train_steps_per_second': 2.897, 'total_flos': 3200484804902088.0, 'train_loss': 0.6911669404115884, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"def get_prediction(text):\n    inputs = tokenizer(text, truncation=True,padding=True, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model(**inputs)\n    proba = outputs[0].softmax(1)\n    return proba.argmax().item()","metadata":{"execution":{"iopub.status.busy":"2023-05-27T14:07:25.911442Z","iopub.execute_input":"2023-05-27T14:07:25.911806Z","iopub.status.idle":"2023-05-27T14:07:25.917287Z","shell.execute_reply.started":"2023-05-27T14:07:25.911763Z","shell.execute_reply":"2023-05-27T14:07:25.916339Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(tokenized_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T14:07:25.918712Z","iopub.execute_input":"2023-05-27T14:07:25.919055Z","iopub.status.idle":"2023-05-27T14:07:36.196248Z","shell.execute_reply.started":"2023-05-27T14:07:25.919025Z","shell.execute_reply":"2023-05-27T14:07:36.195257Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 00:10]\n    </div>\n    "},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'eval_runtime': 10.2267,\n 'eval_samples_per_second': 371.381,\n 'eval_steps_per_second': 5.867,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"predictions = df_test['Text'].apply(lambda text: get_prediction(text))\nsample_submission['Sentiment'] = le.inverse_transform(predictions)\nsample_submission.sample(6)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T14:07:55.562012Z","iopub.execute_input":"2023-05-27T14:07:55.562422Z","iopub.status.idle":"2023-05-27T14:08:54.949738Z","shell.execute_reply.started":"2023-05-27T14:07:55.562391Z","shell.execute_reply":"2023-05-27T14:08:54.948528Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                        id Sentiment\n1070  52f04637-8897-423a-b546-88c165b31128   Neutral","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1070</th>\n      <td>52f04637-8897-423a-b546-88c165b31128</td>\n      <td>Neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}